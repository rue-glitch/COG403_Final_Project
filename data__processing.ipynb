{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, os\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "# Set pandas to display 10 rows only\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Parsing\n",
    "1. Read in the JSON files of each month of the year.\n",
    "2. Combine all the files into a single dataframe for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the JSON files\n",
    "directory = '2014data'\n",
    "\n",
    "# List to store DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json') and filename.startswith('RC_2014'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        # Read JSON file into a DataFrame and append it to the list\n",
    "        # data is written in lines separated by endlines like ‘\\n‘, so we set lines=True\n",
    "        df = pd.read_json(filepath, lines=True)\n",
    "        dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames into a single DataFrame\n",
    "reddit_2014_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data selection\n",
    "\n",
    "### Interested columns:\n",
    "- **id** (String): The submission’s identifier, e.g., “5lcgjh”.\n",
    "- **author** (String): The account name of the poster, e.g., “example username”.\n",
    "- **subreddit** (String): Name of the subreddit that the submission is posted. Note that it excludes the prefix /r/. E.g., ’AskReddit’.\n",
    "- **subreddit id** (String): The identifier of the subreddit, e.g., “t5 2qh1i”.\n",
    "- **created_utc** (Integer): Timestamp indicating when the post was made.\n",
    "\n",
    "Things to keep in mind:\n",
    "**link_id** (String) -  The identifier of the ID of the parent submission of a comment.\n",
    "\n",
    "## Data Cleaning\n",
    "1. Select only the columns of interest.\n",
    "2. Remove all rows with '[deleted]' body, body length of less than 50 words. Rename the body column into 'utterance'\n",
    "4. Remove all the emojies and urls from the body\n",
    "5. Convert the removal reason and distinguished columns into boolean values. If removal reason was not included in the dataset, make it a null value.\n",
    "6. Parse through the body to identify the language and keep only English entries - This may not be a 100% accurate.\n",
    "7. Create a csv file for the cleaned dataframe.\n",
    "8. Repeat this process for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = reddit_2014_df[['parent_id','author','subreddit','created_utc','subreddit_id','body','id']]\n",
    "# Create a dictionary mapping subreddit names to subreddit ids\n",
    "subreddit_dict = dict(zip(df['subreddit_id'], df['subreddit']))\n",
    "# drop all rows that contain '[deleted]'\n",
    "df = df.drop(df[df['body'] == '[deleted]'].index)\n",
    "# remove all body with length < 50 words\n",
    "df = df[df['body'].str.findall(r'\\w+').str.len() >= 50]\n",
    "\n",
    "# Function to remove emojis using regex\n",
    "def remove_emojis(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "# Function to remove URLs using regex\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'^ttps?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    url_pattern = re.compile(r'ttps?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "\n",
    "# Apply the functions to remove emojis and URLs to the 'text' column\n",
    "df['body'] = df['body'].apply(remove_emojis)\n",
    "df['body'] = df['body'].apply(remove_urls)\n",
    "\n",
    "df.rename(columns = {'body':'utterance'}, inplace = True)\n",
    "df.dtypes\n",
    "\n",
    "# replace \"null\" values with False and all other values with True\n",
    "df['distinguished'] = df['distinguished'].fillna(False).astype(bool)\n",
    "# replace \"null\" values with False and all other values with True\n",
    "df['removal_reason'] = df['removal_reason'].fillna(False).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use langdetect to retain only english entries\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return None\n",
    "df['language'] = df['utterance'].apply(detect_language)\n",
    "\n",
    "df = df.drop(df[df['language'] != 'en'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove language column\n",
    "del df['language']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to a csv\n",
    "df.to_csv('RC_2014_counts.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
