{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Set pandas to display 10 rows only\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import & Parsing\n",
    "Importing the LIWC Processed data files across the years and concat them into 1 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliwc_Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRC_2014_18_counts_liwc.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliwc_Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRC_2019_counts_liwc.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df1, df2])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df1 = pd.read_csv('liwc_Data\\RC_2014_18_counts_liwc.csv')\n",
    "df2 = pd.read_csv('liwc_Data\\RC_2019_counts_liwc.csv')\n",
    "\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "# Dictionary to map original column headings to new column headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = {'A': 'parent_id',\n",
    "                  'B': 'author',\n",
    "                  'C': 'subreddit',\n",
    "                  'D': 'created_utc',\n",
    "                  'E': 'controversiality',\n",
    "                  'F': 'subreddit_id', \n",
    "                  'G': 'distinguished',\n",
    "                  'H': 'id',\n",
    "                  'I': 'removal',\n",
    "                  'J': 'utterance'}\n",
    "\n",
    "# Rename columns using the dictionary\n",
    "df.rename(columns=column_mapping, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by province\n",
    "grouped = df.groupby('subreddit')\n",
    "\n",
    "# Create a dictionary to store DataFrames for each subreddit\n",
    "subreddit_dfs = {}\n",
    "\n",
    "# Iterate over groups\n",
    "for subreddit, group_df in grouped:\n",
    "    # Store each group DataFrame in the dictionary\n",
    "    subreddit_dfs[subreddit] = group_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the data\n",
    "table_data = []\n",
    "\n",
    "# Iterate over each subreddit\n",
    "for subreddit in subreddit_dfs:\n",
    "    # Calculate the mean values\n",
    "    mean_i = np.mean(subreddit_dfs[subreddit]['i'])\n",
    "    mean_ppron = np.mean(subreddit_dfs[subreddit]['ppron'])\n",
    "    mean_we = np.mean(subreddit_dfs[subreddit]['we'])\n",
    "    mean_tone = np.mean(subreddit_dfs[subreddit]['Tone'])\n",
    "    mean_posemo = np.mean(subreddit_dfs[subreddit]['posemo'])\n",
    "    mean_negemo = np.mean(subreddit_dfs[subreddit]['negemo'])\n",
    "    \n",
    "    # Append the data to the list\n",
    "    table_data.append([subreddit, mean_i, mean_ppron, mean_we, mean_tone, mean_posemo, mean_negemo])\n",
    "\n",
    "# Define the headers\n",
    "headers = ['Subreddit', 'SD of i', 'SD of ppron', 'SD of we', 'SD of tone', 'SD of posemo', 'SD of negemo']\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the data\n",
    "table_data = []\n",
    "\n",
    "# Iterate over each subreddit\n",
    "for subreddit in subreddit_dfs:\n",
    "    # Calculate the sd values\n",
    "    sd_i = np.std(subreddit_dfs[subreddit]['i'])\n",
    "    sd_ppron = np.std(subreddit_dfs[subreddit]['ppron'])\n",
    "    sd_we = np.std(subreddit_dfs[subreddit]['we'])\n",
    "    sd_tone = np.std(subreddit_dfs[subreddit]['Tone'])\n",
    "    sd_posemo = np.std(subreddit_dfs[subreddit]['posemo'])\n",
    "    sd_negemo = np.std(subreddit_dfs[subreddit]['negemo'])\n",
    "    \n",
    "    # Append the data to the list\n",
    "    table_data.append([subreddit, sd_i, sd_ppron, sd_we, sd_tone, sd_posemo, sd_negemo])\n",
    "\n",
    "# Define the headers\n",
    "headers = ['Subreddit', 'SD of i', 'SD of ppron', 'SD of we', 'SD of tone', 'SD of posemo', 'SD of negemo']\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the data\n",
    "table_data = []\n",
    "\n",
    "# Iterate over each subreddit\n",
    "for subreddit in subreddit_dfs:\n",
    "    # Calculate the minimum values\n",
    "    min_i = np.min(subreddit_dfs[subreddit]['i'])\n",
    "    min_ppron = np.min(subreddit_dfs[subreddit]['ppron'])\n",
    "    min_we = np.min(subreddit_dfs[subreddit]['we'])\n",
    "    min_tone = np.min(subreddit_dfs[subreddit]['Tone'])\n",
    "    min_posemo = np.min(subreddit_dfs[subreddit]['posemo'])\n",
    "    min_negemo = np.min(subreddit_dfs[subreddit]['negemo'])\n",
    "    \n",
    "    # Append the data to the list\n",
    "    table_data.append([subreddit, min_i, min_ppron, min_we, min_tone, min_posemo, min_negemo])\n",
    "\n",
    "# Define the headers\n",
    "headers = ['Subreddit', 'Min of i', 'Min of ppron', 'Min of we', 'Min of tone', 'Min of posemo', 'Min of negemo']\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the data\n",
    "table_data = []\n",
    "\n",
    "# Iterate over each subreddit\n",
    "for subreddit in subreddit_dfs:\n",
    "    # Calculate the maximum values\n",
    "    max_i = np.max(subreddit_dfs[subreddit]['i'])\n",
    "    max_ppron = np.max(subreddit_dfs[subreddit]['ppron'])\n",
    "    max_we = np.max(subreddit_dfs[subreddit]['we'])\n",
    "    max_tone = np.max(subreddit_dfs[subreddit]['Tone'])\n",
    "    max_posemo = np.max(subreddit_dfs[subreddit]['posemo'])\n",
    "    max_negemo = np.max(subreddit_dfs[subreddit]['negemo'])\n",
    "    \n",
    "    # Append the data to the list\n",
    "    table_data.append([subreddit, max_i, max_ppron, max_we, max_tone, max_posemo, max_negemo])\n",
    "\n",
    "# Define the headers\n",
    "headers = ['Subreddit', 'Max of i', 'Max of ppron', 'Max of we', 'Max of tone', 'Max of posemo', 'Max of negemo']\n",
    "\n",
    "# Print the table\n",
    "print(tabulate(table_data, headers=headers, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi2 Tests: Depression vs Yoga\n",
    "These tests are mainly used to see if our findings replicate that of related work and convergences on the markers (positive emotion words, negative emotions words,\n",
    "and pronoun usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2 Test for significance between yoga and depression for all LIWC categories\n",
    "#  we are interested in \n",
    "categories = df.columns[10:]\n",
    "subreddits = [\"depression\", \"yoga\"]\n",
    "data = {'depression': {},\n",
    "        'yoga': {}}\n",
    "for category in categories:\n",
    "    category_data = []\n",
    "    for subreddit in subreddits:\n",
    "        data[subreddit][category] = subreddit_dfs[subreddit][category].sum()\n",
    "contingency_table = pd.DataFrame(data).T\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "print(f\"All categories, Chi-square p-value: {p}\")\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2 Test for significance between yoga and depression for categories we are interested in (EMOTION WORDS)\n",
    "#  we are interested in \n",
    "categories = [\"Tone\", \"posemo\", \"negemo\"]\n",
    "subreddits = [\"depression\", \"yoga\"]\n",
    "\n",
    "data = {'depression': {},\n",
    "        'yoga': {}}\n",
    "for category in categories:\n",
    "    category_data = []\n",
    "    for subreddit in subreddits:\n",
    "        data[subreddit][category] = subreddit_dfs[subreddit][category].sum()\n",
    "contingency_table = pd.DataFrame(data).T\n",
    "\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "print(f\"Category: Tone, posemo, negemo | Chi-square p-value: {p}\")\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chi2 Test for significance between yoga and depression for categories we are interested in (PERSONAL PRONOUNS)\n",
    "#  we are interested in \n",
    "categories = [\"ppron\", \"i\", \"we\",]\n",
    "subreddits = [\"depression\", \"yoga\"]\n",
    "data = {'depression': {},\n",
    "        'yoga': {}}\n",
    "for category in categories:\n",
    "    category_data = []\n",
    "    for subreddit in subreddits:\n",
    "        data[subreddit][category] = subreddit_dfs[subreddit][category].sum()\n",
    "contingency_table = pd.DataFrame(data).T\n",
    "\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "print(f\"Category: ppron, i, we | Chi-square p-value: {p}\")\n",
    "contingency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subreddit_dfs['depression']\n",
    "# Convert UTC timestamp to pandas datetime object\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Extract the year from the timestamp\n",
    "df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "# Group by year and calculate the mean score\n",
    "mean_scores_by_year = df.groupby('year')['i'].mean()\n",
    "\n",
    "# Create a line plot to visualize mean scores across the years\n",
    "mean_scores_by_year.plot(kind='line', marker='o', linestyle='-')\n",
    "plt.title('Mean Depression SubReddit 1st Person Singular Pronoun Frequencies Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean of 1st Person Singular Pronoun Frequencies')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subreddit_dfs['yoga']\n",
    "# Convert UTC timestamp to pandas datetime object\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Extract the year from the timestamp\n",
    "df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "# Group by year and calculate the mean score\n",
    "mean_scores_by_year = df.groupby('year')['i'].mean()\n",
    "\n",
    "# Create a line plot to visualize mean scores across the years\n",
    "mean_scores_by_year.plot(kind='line', marker='o', linestyle='-')\n",
    "plt.title('Mean Yoga SubReddit 1st Person Singular Pronoun Frequencies Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean of 1st Person Singular Pronoun Frequencies')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subreddit_dfs['depression']\n",
    "# Convert UTC timestamp to pandas datetime object\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Extract the year from the timestamp\n",
    "df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "# Group by year and calculate the mean score\n",
    "mean_scores_by_year = df.groupby('year')['posemo'].mean()\n",
    "\n",
    "# Create a line plot to visualize mean scores across the years\n",
    "mean_scores_by_year.plot(kind='line', marker='o', linestyle='-')\n",
    "plt.title('Mean Depression SubReddit Positive Emotion Frequencies Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean of Positive Emotion Frequencies')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subreddit_dfs['yoga']\n",
    "# Convert UTC timestamp to pandas datetime object\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Extract the year from the timestamp\n",
    "df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "# Group by year and calculate the mean score\n",
    "mean_scores_by_year = df.groupby('year')['posemo'].mean()\n",
    "\n",
    "# Create a line plot to visualize mean scores across the years\n",
    "mean_scores_by_year.plot(kind='line', marker='o', linestyle='-')\n",
    "plt.title('Mean Yoga SubReddit Positive Emotion Frequencies Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean of Positive Emotion Frequencies')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subreddit_dfs['depression']\n",
    "# Convert UTC timestamp to pandas datetime object\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Extract the year from the timestamp\n",
    "df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "# Group by year and calculate the mean score\n",
    "mean_scores_by_year = df.groupby('year')['negemo'].mean()\n",
    "\n",
    "# Create a line plot to visualize mean scores across the years\n",
    "mean_scores_by_year.plot(kind='line', marker='o', linestyle='-')\n",
    "plt.title('Mean Depression SubReddit Negative Emotion Frequencies Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean of Negative Emotion Frequencies')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subreddit_dfs['yoga']\n",
    "# Convert UTC timestamp to pandas datetime object\n",
    "df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "# Extract the year from the timestamp\n",
    "df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "# Group by year and calculate the mean score\n",
    "mean_scores_by_year = df.groupby('year')['posemo'].mean()\n",
    "\n",
    "# Create a line plot to visualize mean scores across the years\n",
    "mean_scores_by_year.plot(kind='line', marker='o', linestyle='-')\n",
    "plt.title('Mean Yoga SubReddit Negative Emotion Frequencies Over Time')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Mean of Negative Emotion Frequencies')\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(subreddit_dfs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces = {'Manitoba': 'Manitoba', 'NovaScotia': 'Nova Scotia', 'PEI': 'Prince Edward Islands', 'Quebec': 'Quebec', 'alberta': 'Alberta', 'britishcolumbia': 'British Columbia','newbrunswickcanada': 'New Brunswick', 'newfoundland': 'New Foundland', 'ontario': 'Ontario', 'saskatchewan': 'Saskatchewan'}\n",
    "frequencies = {'i': '1st Person Singular Pronoun Frequencies', 'posemo': 'Positive Emotion Frequencies','negemo':'Negative Emotion Frequencies'}\n",
    "\n",
    "# Iterate over provinces and frequencies\n",
    "for province_key, province_name in provinces.items():\n",
    "    for freq_key, freq_name in frequencies.items():\n",
    "        # Convert UTC timestamp to pandas datetime object\n",
    "        df = subreddit_dfs[province_key]\n",
    "        df['created_utc'] = pd.to_datetime(df['created_utc'], unit='s')\n",
    "\n",
    "        # Extract the year from the timestamp\n",
    "        df['year'] = df['created_utc'].dt.year\n",
    "\n",
    "        # Group by year and calculate the mean score\n",
    "        mean_scores_by_year = df.groupby('year')[freq_key].mean()\n",
    "\n",
    "        # Create a line plot to visualize mean scores across the years\n",
    "        mean_scores_by_year.plot(kind='line', marker='o', linestyle='-')\n",
    "        plt.title(f'Mean {province_name} SubReddit {freq_name} Over Time')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel(f'Mean of {freq_name}')\n",
    "        plt.grid(True)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression: Province Subreddits\n",
    "Using the markers that we tested in the last section we will train a logistic regression model on the r/depression subreddit data and use it to make prediction on the r/{province} subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for frequencies for each subreddit for the regression\n",
    "subreddit_freq = {}\n",
    "\n",
    "# Define the markers identified from chi-square tests\n",
    "markers = [\"Tone\", \"posemo\", \"negemo\", \"ppron\", \"i\", \"we\"]\n",
    "\n",
    "subset = df[markers]\n",
    "# Iterate over subreddits and populate their frequencies\n",
    "for subreddit in subreddit_dfs.keys():\n",
    "    frequencies = np.zeros((6, ))\n",
    "    for i in range(6):\n",
    "        frequencies[i] = np.sum(subreddit_dfs[subreddit][markers[i]])\n",
    "    subreddit_freq[subreddit] = frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_binary_prediction_target_pairs(subreddit_dfs, markers):\n",
    "    # The training pairs will be as follows\n",
    "    # predicitons - (1 for r/depression, 0 for r/yoga)\n",
    "    # target - (1 for markers meeting threshold determined by mean of r/depression, 0 for markers not meeting threshold)\n",
    "    depression_matrix = np.array(subreddit_dfs['depression'][markers])\n",
    "    yoga_matrix = np.array(subreddit_dfs['yoga'][markers])\n",
    "    \n",
    "    # Find thresholds for markers, operationalized by means of r/subreddit\n",
    "    mean_tone = np.mean(subreddit_dfs['depression']['Tone'])\n",
    "    mean_posemo = np.mean(subreddit_dfs['depression']['posemo'])\n",
    "    mean_negemo = np.mean(subreddit_dfs['depression']['negemo'])\n",
    "    mean_ppron = np.mean(subreddit_dfs['depression']['ppron'])\n",
    "    mean_i = np.mean(subreddit_dfs['depression']['i'])\n",
    "    mean_we = np.mean(subreddit_dfs['depression']['we'])\n",
    "    \n",
    "    # Creating masks for based on individual markers\n",
    "    marker_means = np.array([mean_tone, mean_posemo, mean_negemo, mean_ppron, mean_i, mean_we]) # expect more negative words and personal pronouns but less positive words and more negative tone\n",
    "    depression_mask = depression_matrix > marker_means\n",
    "    yoga_mask = yoga_matrix > marker_means\n",
    "\n",
    "    # Create masks based on total marker presence\n",
    "    markers_present = np.array([False, False, True, True, True, True])\n",
    "    depression_mask1 = np.all(depression_mask == markers_present, axis=1)\n",
    "    yoga_mask1 = np.all(yoga_mask == markers_present, axis=1)\n",
    "    \n",
    "    # Create training targets\n",
    "    depression_targets = np.zeros(depression_mask1.shape)\n",
    "    depression_targets[depression_mask1] = 1\n",
    "    depression_targets[~depression_mask1] = 0\n",
    "    yoga_targets = np.zeros(yoga_mask1.shape)\n",
    "    yoga_targets[yoga_mask1] = 1\n",
    "    yoga_targets[~yoga_mask1] = 0\n",
    "    train_targets = np.hstack((depression_targets, yoga_targets))\n",
    "\n",
    "    # Create training predictors\n",
    "    depression_predictors = np.ones(depression_targets.shape)\n",
    "    yoga_predictors = np.zeros(yoga_targets.shape)\n",
    "    train_predictors = np.hstack((depression_predictors, yoga_predictors))\n",
    "\n",
    "    return train_predictors, train_targets\n",
    "\n",
    "\n",
    "train_predictors, train_targets = create_train_binary_prediction_target_pairs(subreddit_dfs, markers)\n",
    "\n",
    "# GOOD INDICATION: 125 are true for yoga and 18,487 are true for depression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_binary_prediction_target_pairs(subreddit_dfs, subreddit, markers):\n",
    "    # The training pairs will be as follows\n",
    "    # predicitons - (1 for r/depression, 0 for r/yoga)\n",
    "    # target - (1 for markers meeting threshold determined by mean of r/depression, 0 for markers not meeting threshold)\n",
    "    subreddit_matrix = np.array(subreddit_dfs[subreddit][markers])\n",
    "    \n",
    "    # Find thresholds for markers, operationalized by means of r/subreddit\n",
    "    mean_tone = np.mean(subreddit_dfs['depression']['Tone'])\n",
    "    mean_posemo = np.mean(subreddit_dfs['depression']['posemo'])\n",
    "    mean_negemo = np.mean(subreddit_dfs['depression']['negemo'])\n",
    "    mean_ppron = np.mean(subreddit_dfs['depression']['ppron'])\n",
    "    mean_i = np.mean(subreddit_dfs['depression']['i'])\n",
    "    mean_we = np.mean(subreddit_dfs['depression']['we'])\n",
    "    \n",
    "    # Creating masks for based on individual markers\n",
    "    marker_means = np.array([mean_tone, mean_posemo, mean_negemo, mean_ppron, mean_i, mean_we]) # expect more negative words and personal pronouns but less positive words and more negative tone\n",
    "    subreddit_mask = subreddit_matrix > marker_means\n",
    "\n",
    "    # Create masks based on total marker presence\n",
    "    markers_present = np.array([False, False, True, True, True, True])\n",
    "    subreddit_mask1 = np.all(subreddit_mask == markers_present, axis=1)\n",
    "    \n",
    "    # Create training targets\n",
    "    targets = np.zeros(subreddit_mask1.shape)\n",
    "    targets[subreddit_mask1] = 1\n",
    "    targets[~subreddit_mask1] = 0\n",
    "   \n",
    "\n",
    "    # Create training predictors\n",
    "    predictors = np.zeros(targets.shape) # TODO: should this be random\n",
    "\n",
    "    return predictors, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_predictors\n",
    "Y = train_targets\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = sm.Logit(Y, X)\n",
    "logit_result = model.fit()\n",
    "print(logit_result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model on posts from other subreddits\n",
    "for subreddit in subreddit_dfs:\n",
    "    test_predictors, test_targets = create_test_binary_prediction_target_pairs(subreddit_dfs, subreddit, markers)\n",
    "    test_predictors = sm.add_constant(test_predictors)\n",
    "    predictions = logit_result.predict(test_predictors)\n",
    "\n",
    "    # Convert probabilities to binary predictions (0 or 1)\n",
    "    binary_predictions = np.round(predictions, decimals=0)\n",
    "    print(f\"Results for {subreddit}:\")\n",
    "    print(classification_report(test_targets, binary_predictions, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
